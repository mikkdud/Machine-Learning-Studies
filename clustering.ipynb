{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb43d79e",
   "metadata": {},
   "source": [
    "# üß© Klasteryzacja (Clustering)\n",
    "\n",
    "W tym laboratorium pracujemy na danych **MNIST** i badamy:\n",
    "- dob√≥r liczby klastr√≥w dla **KMeans** (k = 8..12) z ocenƒÖ **silhouette score**,\n",
    "- zgodno≈õƒá klasteryzacji (k=10) z etykietami odniesienia poprzez **macierz b≈Çƒôd√≥w**,\n",
    "- dob√≥r parametru **eps** dla **DBSCAN** z wykorzystaniem heurystyki opartej o odleg≈Ço≈õci euklidesowe,\n",
    "- eksperyment z r√≥≈ºnymi warto≈õciami `eps` i analizƒô liczby wykrytych klastr√≥w.\n",
    "\n",
    "Zgodnie z poleceniem zapisujemy pliki wynikowe:\n",
    "- `kmeans_sil.pkl`, `kmeans_argmax.pkl`,\n",
    "- `dist.pkl`, `dbscan_len.pkl`.\n",
    "\n",
    "> Notebook ma uk≈Çad i styl sp√≥jny z Twoimi poprzednimi notatnikami (kr√≥tkie wprowadzenie + sekcje z kodem).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae540dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68930eee",
   "metadata": {},
   "source": [
    "## üìä Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd450781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie zbioru MNIST (784 cechy na obserwacjƒô)\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X = mnist['data']\n",
    "y = mnist['target']\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c1ca8",
   "metadata": {},
   "source": [
    "## üî∂ KMeans: dob√≥r liczby klastr√≥w i silhouette score (k=8..12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_to_sil = {}\n",
    "\n",
    "# Dla k = 8..12 trenujemy KMeans (n_init=10) i zapisujemy silhouette score\n",
    "for k in range(8, 13):\n",
    "    km = KMeans(n_clusters=k, n_init=10)\n",
    "    labels = km.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels)\n",
    "    k_to_sil[k] = sil\n",
    "    print(f\"k={k}: silhouette={sil:.5f}\")\n",
    "\n",
    "# Wizualizacja\n",
    "plt.plot(list(k_to_sil.keys()), list(k_to_sil.values()), marker='o')\n",
    "plt.title(\"Silhouette score vs liczba klastr√≥w (KMeans)\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Zapis do Pickle (lista warto≈õci zgodnie z zakresem k=8..12)\n",
    "sil_list = [k_to_sil[k] for k in range(8, 13)]\n",
    "with open('kmeans_sil.pkl', 'wb') as f:\n",
    "    pickle.dump(sil_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13138ef1",
   "metadata": {},
   "source": [
    "## üìê Macierz b≈Çƒôd√≥w dla KMeans (k=10) vs etykiety odniesienia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasteryzacja dla k=10 i por√≥wnanie z y (etykiety referencyjne)\n",
    "km_10 = KMeans(n_clusters=10, n_init=10)\n",
    "y_pred = km_10.fit_predict(X)\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot(xticks_rotation='vertical', colorbar=False)\n",
    "plt.title(\"Macierz b≈Çƒôd√≥w: KMeans (k=10) vs y\")\n",
    "plt.show()\n",
    "\n",
    "# Dla ka≈ºdego wiersza bierzemy indeks kolumny o najwiƒôkszej warto≈õci\n",
    "max_indexes = [int(np.argmax(row)) for row in cm]\n",
    "unique_sorted = sorted(set(max_indexes))\n",
    "print(\"Unikalne indeksy kolumn (posortowane):\", unique_sorted)\n",
    "\n",
    "with open('kmeans_argmax.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_sorted, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fde4f5",
   "metadata": {},
   "source": [
    "## üåÄ DBSCAN: heurystyka doboru `eps` na podstawie odleg≈Ço≈õci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb49cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heurystyka eps:\n",
    "# Liczymy odleg≈Ço≈õci euklidesowe miƒôdzy pierwszymi 300 punktami a wszystkimi innymi.\n",
    "X_first = X[:300]\n",
    "all_dists = []\n",
    "\n",
    "for i in range(X_first.shape[0]):\n",
    "    d = np.linalg.norm(X - X_first[i], axis=1)\n",
    "    d = d[d != 0]  # pomijamy dok≈ÇadnƒÖ odleg≈Ço≈õƒá 0 do samego siebie\n",
    "    all_dists.append(d)\n",
    "\n",
    "all_dists = np.concatenate(all_dists)\n",
    "top_10 = np.sort(all_dists)[:10]\n",
    "print(\"10 najmniejszych odleg≈Ço≈õci:\", top_10)\n",
    "\n",
    "with open('dist.pkl', 'wb') as f:\n",
    "    pickle.dump(top_10, f)\n",
    "\n",
    "# ≈örednia s z 3 najmniejszych warto≈õci\n",
    "s = float(np.mean(top_10[:3]))\n",
    "print(\"s (≈õrednia z 3 najmniejszych):\", s)\n",
    "\n",
    "# eps od s do s + 10%*s z krokiem co 4%*s\n",
    "eps_list = []\n",
    "eps = s\n",
    "while eps < s + 0.10 * s + 1e-12:  # ma≈Çy margines numeryczny\n",
    "    eps_list.append(eps)\n",
    "    eps += 0.04 * s\n",
    "print(\"Testowane eps:\", eps_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858acf",
   "metadata": {},
   "source": [
    "## üîé DBSCAN: eksperyment ‚Äî liczba klastr√≥w dla r√≥≈ºnych `eps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels_counts = []\n",
    "for eps in eps_list:\n",
    "    db = DBSCAN(eps=eps)\n",
    "    db.fit(X)\n",
    "    n_unique = len(np.unique(db.labels_))\n",
    "    unique_labels_counts.append(n_unique)\n",
    "    print(f\"eps={eps:.6f} -> liczba unikalnych etykiet: {n_unique}\")\n",
    "\n",
    "with open('dbscan_len.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_labels_counts, f)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
