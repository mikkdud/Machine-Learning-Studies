{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef419cb",
   "metadata": {},
   "source": [
    "# ü§ñ Metody zespo≈Çowe (Ensemble Methods)\n",
    "\n",
    "W tym laboratorium poznajemy **metody zespo≈Çowe (ensemble methods)**, kt√≥re ≈ÇƒÖczƒÖ wiele modeli bazowych, aby zwiƒôkszyƒá dok≈Çadno≈õƒá predykcji.\n",
    "\n",
    "### Zakres ƒáwiczenia\n",
    "- R√≥wnoleg≈Çe i sekwencyjne metody zespo≈Çowe\n",
    "- **Hard / Soft Voting**\n",
    "- **Bagging** i jego warianty (z i bez bootstrapu)\n",
    "- **Random Forest**\n",
    "- **AdaBoost**\n",
    "- **Gradient Boosting**\n",
    "- Sampling cech i ranking dok≈Çadno≈õci estymator√≥w\n",
    "\n",
    "Celem ƒáwiczenia jest zrozumienie, jak ≈ÇƒÖczenie klasyfikator√≥w wp≈Çywa na dok≈Çadno≈õƒá i generalizacjƒô modelu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8221198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da065f",
   "metadata": {},
   "source": [
    "## üìä Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081750ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_breast_cancer = datasets.load_breast_cancer(as_frame=True)\n",
    "X = data_breast_cancer['data'][['mean texture', 'mean symmetry']]\n",
    "y = data_breast_cancer['target']\n",
    "\n",
    "# Podzia≈Ç 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Zbi√≥r treningowy: {X_train.shape}, Zbi√≥r testowy: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2af2d",
   "metadata": {},
   "source": [
    "## üó≥Ô∏è Hard / Soft Voting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasyfikatory bazowe\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# --- Hard Voting ---\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('tree', tree_clf), ('knn', knn_clf)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "hard_train_acc = accuracy_score(y_train, voting_clf.predict(X_train))\n",
    "hard_test_acc = accuracy_score(y_test, voting_clf.predict(X_test))\n",
    "print(f\"Hard Voting - train: {hard_train_acc:.4f}, test: {hard_test_acc:.4f}\")\n",
    "\n",
    "# --- Soft Voting ---\n",
    "voting_clf_soft = VotingClassifier(estimators=[('lr', log_clf), ('tree', tree_clf), ('knn', knn_clf)], voting='soft')\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "\n",
    "soft_train_acc = accuracy_score(y_train, voting_clf_soft.predict(X_train))\n",
    "soft_test_acc = accuracy_score(y_test, voting_clf_soft.predict(X_test))\n",
    "print(f\"Soft Voting - train: {soft_train_acc:.4f}, test: {soft_test_acc:.4f}\")\n",
    "\n",
    "# --- Accuracy pojedynczych modeli ---\n",
    "tree_clf.fit(X_train, y_train)\n",
    "log_clf.fit(X_train, y_train)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "\n",
    "acc_list = [\n",
    "    (accuracy_score(y_train, tree_clf.predict(X_train)), accuracy_score(y_test, tree_clf.predict(X_test))),\n",
    "    (accuracy_score(y_train, log_clf.predict(X_train)), accuracy_score(y_test, log_clf.predict(X_test))),\n",
    "    (accuracy_score(y_train, knn_clf.predict(X_train)), accuracy_score(y_test, knn_clf.predict(X_test))),\n",
    "    (hard_train_acc, hard_test_acc),\n",
    "    (soft_train_acc, soft_test_acc)\n",
    "]\n",
    "\n",
    "with open(\"acc_vote.pkl\", \"wb\") as f:\n",
    "    pickle.dump(acc_list, f)\n",
    "\n",
    "# Zapisanie klasyfikator√≥w\n",
    "clf_list = [tree_clf, log_clf, knn_clf, voting_clf, voting_clf_soft]\n",
    "with open(\"vote.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf10b69",
   "metadata": {},
   "source": [
    "## üß© Bagging i warianty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging - podstawowy\n",
    "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=30, bootstrap=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "# Bagging 50%\n",
    "bag_clf_50 = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=30, bootstrap=True, max_samples=0.5, random_state=42)\n",
    "bag_clf_50.fit(X_train, y_train)\n",
    "\n",
    "# Pasting\n",
    "pas_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=30, bootstrap=False, random_state=42)\n",
    "pas_clf.fit(X_train, y_train)\n",
    "\n",
    "# Pasting 50%\n",
    "pas_clf_50 = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=30, bootstrap=False, max_samples=0.5, random_state=42)\n",
    "pas_clf_50.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest, AdaBoost, Gradient Boosting\n",
    "rfc = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=30, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=30, random_state=42)\n",
    "\n",
    "for model in [rfc, ada_clf, gb_clf]:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Zapis wynik√≥w\n",
    "models = [bag_clf, bag_clf_50, pas_clf, pas_clf_50, rfc, ada_clf, gb_clf]\n",
    "acc_list2 = [(accuracy_score(y_train, m.predict(X_train)), accuracy_score(y_test, m.predict(X_test))) for m in models]\n",
    "\n",
    "with open(\"acc_bag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(acc_list2, f)\n",
    "\n",
    "with open(\"bag.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "acc_list2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec256a",
   "metadata": {},
   "source": [
    "## üéØ Sampling cech i ranking estymator√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling 2 cech bez powt√≥rze≈Ñ\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(data_breast_cancer.data, data_breast_cancer.target, test_size=0.2, random_state=42)\n",
    "\n",
    "bag_sam = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=30,\n",
    "    max_features=2,\n",
    "    bootstrap=True,\n",
    "    max_samples=0.5,\n",
    "    bootstrap_features=False\n",
    ")\n",
    "bag_sam.fit(X_train2, y_train2)\n",
    "\n",
    "# Dok≈Çadno≈õƒá dla ca≈Çego zespo≈Çu\n",
    "acc_list3 = [accuracy_score(y_train2, bag_sam.predict(X_train2)), accuracy_score(y_test2, bag_sam.predict(X_test2))]\n",
    "with open(\"acc_fea.pkl\", \"wb\") as f:\n",
    "    pickle.dump(acc_list3, f)\n",
    "\n",
    "with open(\"fea.pkl\", \"wb\") as f:\n",
    "    pickle.dump([bag_sam], f)\n",
    "\n",
    "# Ranking dok≈Çadno≈õci dla poszczeg√≥lnych estymator√≥w\n",
    "df_rank = pd.DataFrame(columns=['train_accuracy', 'test_accuracy', 'features'])\n",
    "for i, est in enumerate(bag_sam.estimators_):\n",
    "    features = bag_sam.estimators_features_[i]\n",
    "    feature_names = data_breast_cancer.data.columns[features]\n",
    "    train_acc = accuracy_score(y_train2, est.predict(X_train2[feature_names].values))\n",
    "    test_acc = accuracy_score(y_test2, est.predict(X_test2[feature_names].values))\n",
    "    df_rank.loc[len(df_rank)] = [train_acc, test_acc, feature_names.values]\n",
    "\n",
    "df_rank.sort_values(by=['test_accuracy', 'train_accuracy'], ascending=False, inplace=True)\n",
    "\n",
    "with open(\"acc_fea_rank.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_rank, f)\n",
    "\n",
    "df_rank.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
